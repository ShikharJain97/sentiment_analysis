{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, metrics, svm,model_selection\n",
    "from IPython.display import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline  \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...</td>\n",
       "      <td>The sales people are really nice and knowledge...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...</td>\n",
       "      <td>Great product and service and the sales man r ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...</td>\n",
       "      <td>Great product and service will refer to a frie...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ProductName  \\\n",
       "0  Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...   \n",
       "1  Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...   \n",
       "2  Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...   \n",
       "\n",
       "                                             Reviews  Rating  \n",
       "0  The sales people are really nice and knowledge...     5.0  \n",
       "1  Great product and service and the sales man r ...     5.0  \n",
       "2  Great product and service will refer to a frie...     5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data=pd.read_csv(\"/home/dexterslaj/FireTablet1.csv\")\n",
    "review_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sales people are really nice and knowledge...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product and service and the sales man r ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great product and service will refer to a frie...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Rating\n",
       "0  The sales people are really nice and knowledge...     5.0\n",
       "1  Great product and service and the sales man r ...     5.0\n",
       "2  Great product and service will refer to a frie...     5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.drop('ProductName',1,inplace=True)\n",
    "review_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dexterslaj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in range(10961):\n",
    "    x.append(str(i))\n",
    "\n",
    "    \n",
    "nltk.download('punkt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "vocabulary={'null'}\n",
    "print(type(vocabulary))\n",
    "for i in range(10961):\n",
    "    temp_review=review_data.iloc[[i]]\n",
    "    temp_message=str(temp_review[['Reviews']])\n",
    "   # print(type(temp_message))\n",
    "    tempwords=word_tokenize(temp_message)\n",
    "    for w in tempwords:\n",
    "        if  w  in x:\n",
    "            tempwords.remove(w)\n",
    "   # print (tempwords)  \n",
    "    for w in tempwords:\n",
    "        vocabulary |= set(tempwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5477\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a word  \"agyat\" (hindi) which is \"unknown\" in english hence this \"agyat\" will serve as unknown category..\n",
    "vocabulary.add('agyat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5478\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# stopword removal\n",
    "stopwordlist=list(stopwords.words('english'))\n",
    "print(stopwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordslist1=[]\n",
    "for i in range(116):\n",
    "    stopwordslist1.append(stopwordlist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such']\n"
     ]
    }
   ],
   "source": [
    "print(stopwordslist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopvocab=[]\n",
    "for w in vocabulary:\n",
    "   # print(w)\n",
    "    if w in stopwordslist1:\n",
    "        continue\n",
    "    else:    \n",
    "        stopvocab.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5376\n"
     ]
    }
   ],
   "source": [
    "print(len(stopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for w in stopvocab:\n",
    "    stopvocab[i]=w.lower()\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing digits and special charachters\n",
    "j=0\n",
    "for w in stopvocab:\n",
    "    i=0\n",
    "    temp=list(w)\n",
    "    for c in temp:\n",
    "        if c>='a' and c<='z':\n",
    "            continue\n",
    "        else:\n",
    "            temp[i]=''\n",
    "        i=i+1    \n",
    "    stopvocab[j]=\"\".join(temp)\n",
    "    temp=[]\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5376\n"
     ]
    }
   ],
   "source": [
    "print(len(stopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing non english words\n",
    "# SAD NEWS THAT SUPPORT FOR PYENCHANT HAS BEEN ENDED FOR ANACONDA ENVIROMENT SO NOW WE HAVE TO USE AN ALTERNATIVE IN ORDER TO REMOVE NON-ENGLISH WORDS\n",
    "#import pyenchant\n",
    "#d=enchant.Dict(\"en-us\")\n",
    "\n",
    "#enstopvocab=[]\n",
    "#for w in stopvocab:\n",
    "    #enstopvocab.append(w)\n",
    "    \n",
    "#for w in stopvocab:\n",
    "    \n",
    "    #if w!=\"\" and d.check(w)==False:\n",
    "        #enstopvocab.remove(w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "enstopvocab=[]\n",
    "for w in stopvocab:\n",
    "    enstopvocab.append(w)\n",
    "for w in stopvocab:\n",
    "    if w!=\"\" and  not wordnet.synsets(w):\n",
    "        enstopvocab.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587\n"
     ]
    }
   ],
   "source": [
    "print(len(enstopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n"
     ]
    }
   ],
   "source": [
    "enstopvocab=list(set(enstopvocab))\n",
    "print(len(enstopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oops our added word was removed since it was not an english word hence readding.\n",
    "enstopvocab.append('agyat')\n",
    "#thus the last position or index is for 'agyat' or unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(enstopvocab[0]) now here is a problem that first word in enstopvocab is null so we have to remove it\n",
    "enstopvocab.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n"
     ]
    }
   ],
   "source": [
    "print(len(enstopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making vectors 1) one hot encoding 2) count vector\n",
    "# using vovab enstopvocab\n",
    "# using one-hot encodeing\n",
    "def messagetoonehotvector(x):    \n",
    "    messagevector=np.zeros(3620,dtype=int)\n",
    "    temp1=word_tokenize(x)\n",
    "    temp2=[]\n",
    "    #for x in temp1:\n",
    "       # x=lemmatizer.lemmatize(x)\n",
    "        #temp2.append(x)\n",
    "    for x in temp1:\n",
    "        if x in stopwordslist:\n",
    "            temp1.remove(x)\n",
    "            \n",
    "    for x in temp1:\n",
    "        if x in enstopcovab:\n",
    "            i=enstopvoab.index(x)\n",
    "            messagevector[i]=1\n",
    "            \n",
    "            \n",
    "    return list(messagevector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect_esv=feature_extraction.text.CountVectorizer(vocabulary=enstopvocab)\n",
    "tfidfvect=feature_extraction.text.TfidfVectorizer()\n",
    "tfidfvect_esv=feature_extraction.text.TfidfVectorizer(vocabulary=enstopvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv=countvect_esv.transform(review_data['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10962, 2645)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = model_selection.train_test_split(X_cv, review_data['Rating'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "101\n",
      "458\n",
      "2348\n",
      "4326\n"
     ]
    }
   ],
   "source": [
    "#there is nothing in file so no NAN entry\n",
    "x=np.array(y_train_cv)\n",
    "count1=0\n",
    "count2=0\n",
    "count3=0\n",
    "count4=0\n",
    "count5=0\n",
    "for p in x:\n",
    "    if p==1:\n",
    "        count1+=1\n",
    "    elif p==2:\n",
    "        count2+=1\n",
    "    elif p==3:\n",
    "        count3+=1\n",
    "    elif p==4:\n",
    "        count4+=1\n",
    "    elif p==5:\n",
    "        count5+=1\n",
    "    \n",
    "print(count1)\n",
    "print(count2)\n",
    "print(count3)\n",
    "print(count4)\n",
    "print(count5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myfile1=open(\"SentiWordNet_3.0.0.txt\",'r')\n",
    "#myfile2=open(\"polaritydatabase.csv\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer2=csv.writer(myfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_header=['POS','ID','POSSCORE','NEGSCORE','SYNSETTERMS','GLOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer2.writerow(csv_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer2.writerow(['popo','2222'])\n",
    "ssyn1=open('sentiwordnetsynsets1.txt','w')# this file will contain all the synsets for each word from enstopvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continues\n",
      "2645\n"
     ]
    }
   ],
   "source": [
    "print(enstopvocab[0])\n",
    "#enstopvocab.remove('')\n",
    "print(len(enstopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n"
     ]
    }
   ],
   "source": [
    "enstopvocab=list(set(enstopvocab))\n",
    "print(len(enstopvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "polscore2=open('polscores2.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempsyn=[]\n",
    "for w in enstopvocab:\n",
    "    tempsyn=swn.senti_synsets(w)\n",
    "    #polscore2.write(\"word is \"+ str(w)+\" \")\n",
    "    #if len(list(tempsyn))==0:\n",
    "        #ssyn1.write((str(x)+\"  \"))\n",
    "        #polscore2.write((str(0.05)+\" \"+str(0.04)+\",\"))\n",
    "    for x in tempsyn:        \n",
    "        ssyn1.write((str(x)+\"  \"))\n",
    "        polscore2.write((str(x.pos_score())+\" \"+str(x.neg_score())+\",\"))\n",
    "    ssyn1.write(\"\\n\")\n",
    "    polscore2.write(\"\\n\")                    \n",
    "    tempsyn=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssyn1.close()# finally we have a record of all synsets for ech word in enstopvocab\n",
    "polscore2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A BIG NOTE :----- WE ARE USING SENTI WORD NET INTERFACE OF NLTK THAT COMPLETELY RELIES UPON SENTI WORD NET DATABASE.\n",
    "# A QUICK OVERVIEW\n",
    "# CURRENTLY WE ARE FINDING THE POLARITY OF EACH WORD\n",
    "# NOW AS WE KNPOW THAT EACH WORD HAS SEVERAL SYNSETS DEPENDING ON PART OF SPEECH , CONTEXTUAL USE ETC.\n",
    "# SINCE THIS IS OUR FIRST APPROACH TOWARDS POLARITY CLACULATION SO WE ARE JUST TAKING AREITHMATIC AVERAGE OVER ALL THE SYNSETS OF A WORD REGARDLESS OF ITS POS  OR CONTEXTUAL USE\n",
    "# WE MAY GO FOR FURTHER ADVANCEMENT OF OUR ALGO..\n",
    "# the approach we are using is not weaker in any sense . TEXTBLOB a famous sentiment analysis library uses this approach\n",
    "avgposscore=[]# to hold avg pos polarity score of each word\n",
    "avgnegscore=[] # to hold avg neg polarity score of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "temppos=0.0\n",
    "tempneg=0.0\n",
    "count=0\n",
    "polscore2=open('polscores2.txt','r')\n",
    "temp=\"\"\n",
    "templist=[]\n",
    "ptemp=[]\n",
    "tempsyn=[]\n",
    "i=1\n",
    "for temp in polscore2:\n",
    "    #for line in enstopvocab:\n",
    "    templist=temp.split(',')\n",
    "    #if i<=10:\n",
    "    #print(str(i)+\"):--\")\n",
    "    #print(temp)\n",
    "    #print(\"\\n\")\n",
    "    #i=i+1    \n",
    "    #tempsyn=swn.senti_synsets(x)\n",
    "    #count=len(list(tempsyn))# toatl no. of synsets returned\n",
    "    for x in templist:\n",
    "        if len(x)!=0:\n",
    "            ptemp=x.split()\n",
    "            #if i<=10:        \n",
    "            #print(ptemp)\n",
    "            #print(\"\\n\")\n",
    "            if len(ptemp)!=0:\n",
    "                temppos=temppos+float(ptemp[0])\n",
    "                tempneg=tempneg+float(ptemp[1])\n",
    "            ptemp=[]\n",
    "        \n",
    "    #print (count)\n",
    "    #print(type(tempsyn))\n",
    "    #print(tempsyn)\n",
    "   # for w in tempsyn:\n",
    "        \n",
    "       # print(w.pos_score())\n",
    "        #temppos=temppos+w.pos_score()# total pos score\n",
    "        #tempneg=tempneg+w.neg_score()# total neg score\n",
    "    #print(tempos)\n",
    "    #print(tempneg)\n",
    "    #tempos=temppos/count\n",
    "    #tempneg=tempneg/count\n",
    "    avgposscore.append(temppos)\n",
    "    avgnegscore.append(tempneg)\n",
    "    temppos=0.0\n",
    "    tempneg=0.0\n",
    "    count=0\n",
    "    tempsyn=[]\n",
    "    templist=[]\n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now recording these avg polarity scores\n",
    "#polscore1=open('polscores1.txt','w')\n",
    "#polscore1.write(\"pos score                neg score\")\n",
    "#for x in range(2641):\n",
    "    #polscore1.write((str(avgposscore[x])+\"       \"+str(avgnegscore[x])))\n",
    "    #polscore1.write(\"\\n\")\n",
    "\n",
    "#print(avgposscore)\n",
    "count=0\n",
    "for x in range(2642):\n",
    "    if enstopvocab[x]!='agyat':\n",
    "        temp=swn.senti_synsets(enstopvocab[x])\n",
    "        count=len(list(temp))\n",
    "        #print(count)\n",
    "        if count==0:\n",
    "            print(enstopvocab[x])\n",
    "        avgposscore[x]=avgposscore[x]/count\n",
    "        avgnegscore[x]=avgnegscore[x]/count\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognizing\n",
      "0.16666666666666666\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(enstopvocab[2502])\n",
    "print(avgposscore[2502])\n",
    "print(avgnegscore[2502])\n",
    "#print(avgposscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# now multiplying polarity with X_cv\n",
    "print(type(X_cv))\n",
    "fpol=np.zeros(shape=2642,dtype=float)\n",
    "np.shape(fpol)\n",
    "for i in range(2642):\n",
    "    fpol[i]=avgposscore[i]+((-1)*avgnegscore[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.0625 ... 0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "print(fpol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv1=X_cv.toarray()\n",
    "#print(X_cv1)\n",
    "#for i in range(10962):\n",
    "    #for j in range(2642):\n",
    "        #X_cv1[i][j]=(X_cv1[i][j]*fpol[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = model_selection.train_test_split(X_cv1, review_data['Rating'], test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=2645, activation='relu'))\n",
    "#model.add(Dense(4000, activation='softmax'))\n",
    "model.add(Dense(500,activation='relu'))\n",
    "#model.add(Dense(150, activation='softmax'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7344, 2645)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now making a matrix of size 10962*5 which will serve as a y (train label vectors) according to which the model will train\n",
    "y_vec=np.zeros((10962,5),dtype=float)\n",
    "for i in range(10962):\n",
    "    for j in range(5):\n",
    "        p=review_data['Rating'][i]\n",
    "        #print(p)\n",
    "        #y_vec[i][0]=p\n",
    "        #y_vec[i][1]=p\n",
    "        #y_vec[i][2]=p\n",
    "        #y_vec[i][3]=p\n",
    "        #y_vec[i][4]=p\n",
    "        y_vec[i][int(p)-1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_cv, X_test_cv, y_train_cv, y_test_cv = model_selection.train_test_split(X_cv1, y_vec, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7344, 5)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dexterslaj/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "7344/7344 [==============================] - 50s 7ms/step - loss: 0.8922 - acc: 0.6121\n",
      "Epoch 2/4\n",
      "7344/7344 [==============================] - 49s 7ms/step - loss: 0.6514 - acc: 0.7102\n",
      "Epoch 3/4\n",
      "7344/7344 [==============================] - 51s 7ms/step - loss: 0.3219 - acc: 0.8697\n",
      "Epoch 4/4\n",
      "7344/7344 [==============================] - 54s 7ms/step - loss: 0.1084 - acc: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00a3467e10>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_cv,y_train_cv,epochs=4,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input sentence s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "#input a sentence\n",
    "sen=input(\"input sentence\")\n",
    "print(sen)\n",
    "#now first we have to change this sentence to count vector\n",
    "#since this sentence may contain some unknown words so we haveto take care of them also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizatin\n",
    "inp_vec=np.zeros(shape=2642,dtype=int)\n",
    "for i in range(2642):\n",
    "    p=enstopvocab[i]\n",
    "    for w in sen:\n",
    "        if w==p:\n",
    "            inp_vec[i]=inp_vec[i]+1\n",
    "        else:\n",
    "            inp_vec[2502]=inp_vec[2502]+1 # for \"agyat\" or 'unknown' category of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate it, there isn't any organization for the apps\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(sen)\n",
    "print(inp_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying with polarity\n",
    "#for i in range(2642):\n",
    "    #inp_vec[i]=inp_vec[i]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2642,)\n",
      "(1, 2642)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(inp_vec))\n",
    "inp_vec_matrix=np.zeros(shape=(1,2642),dtype=float)\n",
    "print(np.shape(inp_vec_matrix))\n",
    "for j in range(2642):\n",
    "        inp_vec_matrix[0][j]=inp_vec[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inp_vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in enstopvocab:\n",
    "    if x=='hategjhgj':\n",
    "        print(\"yea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
